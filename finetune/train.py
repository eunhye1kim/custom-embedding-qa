import os
import json
import wandb
from datasets import load_dataset, Dataset, concatenate_datasets
from dotenv import load_dotenv
from huggingface_hub import login
from sentence_transformers import SentenceTransformer, InputExample, losses, models
from torch.utils.data import DataLoader

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()

# ğŸ’¡ í™˜ê²½ ë³€ìˆ˜
HF_TOKEN = os.getenv("HF_TOKEN")
HF_DATASET_NAME = os.getenv("HF_DATASET_NAME")
HF_USERNAME = "lunara-kim"
HF_MODEL_REPO = "custom-embedding-slang-model"
OUTPUT_PATH = f"./output/{HF_MODEL_REPO}"

# âœ… W&B ì„¤ì •
wandb.init(
    project="custom-embedding-qa",
    name="slang-finetune",
    job_type="training"
)

login(HF_TOKEN)  # .envì—ì„œ ë¶ˆëŸ¬ì˜¨ access token ì‚¬ìš©

# âœ… 1. ë°ì´í„° ë¡œë“œ ë° í™•ì¥
base_ds = load_dataset(HF_DATASET_NAME, split="train")

with open("./slang_scenarios.jsonl", "r", encoding="utf-8") as f:
    slang_data = [json.loads(line) for line in f]
    for x in slang_data:
        x["meta"]["source_id"] = str(x["meta"]["source_id"])  # type ì¼ì¹˜

slang_ds = Dataset.from_list(slang_data * 10)
combined_ds = concatenate_datasets([base_ds, slang_ds])

def to_input_examples(dataset):
    return [
        InputExample(texts=[row["query"], row["positive_context"]])
        for row in dataset
    ]

all_examples = to_input_examples(combined_ds)
slang_examples = to_input_examples(slang_ds)

# âœ… 2. ëª¨ë¸ êµ¬ì„±
model_name = "google-bert/bert-base-multilingual-cased"
word_embedding_model = models.Transformer(model_name)
pooling_model = models.Pooling(
    word_embedding_model.get_word_embedding_dimension(), pooling_mode='mean'
)
model = SentenceTransformer(modules=[word_embedding_model, pooling_model])

# âœ… 3. í•™ìŠµ ì„¤ì •
train_loader_all = DataLoader(all_examples, shuffle=True, batch_size=32)
train_loader_slang = DataLoader(slang_examples, shuffle=True, batch_size=16)

loss_all = losses.MultipleNegativesRankingLoss(model)
loss_slang = losses.MultipleNegativesRankingLoss(model)

model.fit(
    train_objectives=[
        (train_loader_all, loss_all),
        (train_loader_slang, loss_slang)
    ],
    epochs=5,
    warmup_steps=100,
    output_path=OUTPUT_PATH,
    show_progress_bar=True
)

# âœ… 4. Hugging Face ì—…ë¡œë“œ
model.push_to_hub(
    HF_USERNAME + "/" + HF_MODEL_REPO,
    token=HF_TOKEN
)

wandb.finish()

print(f"âœ… ëª¨ë¸ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: https://huggingface.co/{HF_USERNAME}/{HF_MODEL_REPO}")
