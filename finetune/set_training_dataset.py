from datasets import load_dataset, Dataset, concatenate_datasets
import json
from dotenv import load_dotenv
import os

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()

HF_TOKEN = os.getenv("HF_TOKEN")
HF_DATASET_NAME = os.getenv("HF_DATASET_NAME")

# ê²½ë¡œ ì„¤ì •
output_dir = "./finetune/data"
os.makedirs(output_dir, exist_ok=True)

from huggingface_hub import login
login(HF_TOKEN)  # .envì—ì„œ ë¶ˆëŸ¬ì˜¨ access token ì‚¬ìš©

# 1. Hugging Faceì—ì„œ ê¸°ì¡´ split ë¡œë“œ
base_dataset = load_dataset(HF_DATASET_NAME, split="train")
test_dataset = load_dataset(HF_DATASET_NAME, split="test")

# 2. slang_scenarios.jsonl ë¡œë”©
slang_data = []
with open(f"{output_dir}/slang_scenarios.jsonl", "r", encoding="utf-8") as f:
    for line in f:
        item = json.loads(line.strip())
        # ğŸ”§ source_id ê°•ì œ ë¬¸ìì—´ ì²˜ë¦¬
        if isinstance(item["meta"]["source_id"], int):
            item["meta"]["source_id"] = str(item["meta"]["source_id"])
        slang_data.append(item)

slang_ds = Dataset.from_list(slang_data)

# 3. ë³‘í•© (trainë§Œ ëŒ€ìƒ)
train_combined = concatenate_datasets([base_dataset, slang_ds])

# 4. JSONL ì €ì¥ í•¨ìˆ˜
def save_jsonl(dataset: Dataset, path: str):
    with open(path, "w", encoding="utf-8") as f:
        for sample in dataset:
            json.dump(sample, f, ensure_ascii=False)
            f.write("\n")

# 5. ì €ì¥
save_jsonl(train_combined, f"{output_dir}/train.jsonl")
save_jsonl(test_dataset, f"{output_dir}/valid.jsonl")

print("âœ… ì €ì¥ ì™„ë£Œ: ./finetune/data/train.jsonl, ./finetune/data/valid.jsonl")
